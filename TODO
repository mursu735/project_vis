# TODO:
# Add more words (1000 random words) (DONE)
# Spot the exact time (DONE) and location for outbreak start (Add animation for outbreak), 
# Add alpha to map (DONE)
# Spot the first message (?)
# Check for more symptoms
# Heatmap of features, similarity of similarity; create a vector of similarity for all symptoms, then compare the similarity of those vectors (cosine) (DONE, maybe)
# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html?highlight=clustering
# Self organized map
Train two models
Collection of words
Term frequency correlation
Plot different symptoms at different colors; regular flu as well (DONE)
List messages under slider, or something like that (most likely very difficult unless using some other UI library)
Separate texts between normal flu and outbreak (binary classifier, training with messages before and after), ADD LEGEND
Piechart, different symptoms, animation subplots
Summarize by Each district, proportion, bar chart, use edited map to figure out the district, then plot them accordingly based on symptoms (DONE)

NEW:
Find the very first case (probably in Smogtown, Plainville, Downtown)
Write a report so far

Fix barchart y-axis
Split times into coarser periods for bar chart (am, pm, etc.), filter interesting areas, select interesting times, temporal analysis of patterns
Normalize number of symptom-related messages with messages (per space area)
Automate process to find spike, line chart of normalized number of symptoms against the number of total messages, subplots
Compare messages in Smogtown before outbreak

Find the case for each of the three phenomenon (lemmatize words, possibly generate word cloud for different days, look into word sense disambiguation)

Check coordinate 42.20621 93.3021
22953,5/18/2011 5:35,42.20621 93.3021,fuel spill - haz mat - Dept of Interior - 1800 blk C St NW - trk fuel tank ruptured

Check messages 5/17 afternoon near 610, might be spillage

=============================================================================

Take a long text, vectorize it, visualize the embeddings, apply umap and PCA, visualize side by side, select word from list, highlight word and show it in both umap and PCA (DONE! (mainly))
Select cluster of words, print the words in a text box, highlight them in PCA, dbscan over output
Filter words, use tf-idf
Use doc2vec with paragraphs
Color enconding, what chapter is the tf-idf the highest (can be switched by commenting stuff out, adding a switch would make it look better)
Change t-SNE to umap, change dataset from own model to pretrained google 300 vector dataset, but use labels from the trained data (umap done, model can be switched by commenting the line that selects the model)
Apply NLP techniques like nre to extract important events at a glance